# AmbedkarGPT - SEMRAG-based RAG System

A fully functional RAG (Retrieval-Augmented Generation) system for answering questions about Dr. B.R. Ambedkar's works, built following the **SEMRAG research paper** architecture.

## ğŸ¯ Features

- **Semantic Chunking** (Algorithm 1): Groups sentences by cosine similarity with buffer merging
- **Knowledge Graph**: Entities and relationships with community detection (Louvain/Leiden)
- **Local RAG Search** (Equation 4): Entity-based chunk retrieval
- **Global RAG Search** (Equation 5): Community-based retrieval
- **LLM Integration**: Mistral 7B via Ollama for answer generation

## ğŸ“‹ Requirements

- Python 3.9+
- Ollama with Mistral 7B model
- ~4GB RAM for embeddings and graph
- ~8GB VRAM for Mistral 7B (or CPU inference)

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate

# Install dependencies
cd ambedkargpt
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_sm
```

### 2. Ensure Ollama is Running

```bash
# Start Ollama server (in another terminal)
ollama serve

# Verify Mistral is available
ollama list

# Pull if not available
ollama pull mistral:7b
```

### 3. Ingest the Document

```bash
# Run ingestion pipeline
python run.py --ingest

# Or with custom PDF path
python run.py --ingest --pdf "../data/Ambedkar_book.pdf"
```

### 4. Ask Questions

```bash
# Interactive mode
python run.py --interactive

# Single query
python run.py --query "What were Ambedkar's views on caste?"
```

## ğŸ“ Project Structure

```
ambedkargpt/
â”œâ”€â”€ config.yaml              # Configuration file
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ run.py                   # Main entry point
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Ambedkar_book.pdf    # Source document (copy here)
â”‚   â””â”€â”€ processed/           # Processed data (auto-generated)
â”‚       â”œâ”€â”€ chunks.json
â”‚       â”œâ”€â”€ sub_chunks.json
â”‚       â”œâ”€â”€ knowledge_graph.pkl
â”‚       â”œâ”€â”€ embeddings.pkl
â”‚       â””â”€â”€ communities.pkl
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ chunking/            # Semantic chunking (Algorithm 1)
â”‚   â”‚   â”œâ”€â”€ semantic_chunker.py
â”‚   â”‚   â””â”€â”€ buffer_merger.py
â”‚   â”œâ”€â”€ graph/               # Knowledge graph construction
â”‚   â”‚   â”œâ”€â”€ entity_extractor.py
â”‚   â”‚   â”œâ”€â”€ graph_builder.py
â”‚   â”‚   â”œâ”€â”€ community_detector.py
â”‚   â”‚   â””â”€â”€ summarizer.py
â”‚   â”œâ”€â”€ retrieval/           # RAG search (Equations 4 & 5)
â”‚   â”‚   â”œâ”€â”€ local_search.py
â”‚   â”‚   â”œâ”€â”€ global_search.py
â”‚   â”‚   â””â”€â”€ ranker.py
â”‚   â”œâ”€â”€ llm/                 # LLM integration
â”‚   â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”‚   â”œâ”€â”€ prompt_templates.py
â”‚   â”‚   â””â”€â”€ answer_generator.py
â”‚   â””â”€â”€ pipeline/            # Main pipeline
â”‚       â””â”€â”€ ambedkargpt.py
â””â”€â”€ tests/                   # Unit tests
    â”œâ”€â”€ test_chunking.py
    â”œâ”€â”€ test_retrieval.py
    â””â”€â”€ test_integration.py
```

## âš™ï¸ Configuration

Edit `config.yaml` to adjust parameters:

```yaml
# Chunking parameters
chunking:
  embedding_model: "all-MiniLM-L6-v2"
  max_tokens: 1024
  similarity_threshold: 0.5

# Retrieval thresholds
retrieval:
  local:
    entity_similarity_threshold: 0.3  # Ï„_e
    chunk_similarity_threshold: 0.2   # Ï„_d
    top_k: 5

# LLM settings
llm:
  model: "mistral:7b"
  base_url: "http://localhost:11434"
  temperature: 0.7
```

## ğŸ§ª Running Tests

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_chunking.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ“– Usage Examples

### Python API

```python
from src.pipeline.ambedkargpt import AmbedkarGPT

# Initialize
gpt = AmbedkarGPT("config.yaml")

# Load processed data (if already ingested)
gpt.load_processed_data()

# Or ingest new document
gpt.ingest_document("path/to/pdf")

# Query
result = gpt.query("What is Ambedkar's view on education?")
print(result["answer"])
print(result["citations"])
```

### Command Line

```bash
# Ingest document
python run.py --ingest

# Query mode
python run.py --query "Explain Ambedkar's philosophy on equality"

# Interactive chat
python run.py --interactive

# Load and query (skip ingestion)
python run.py --load --query "What reforms did Ambedkar advocate?"
```

## ğŸ¤ Live Demo Checklist

Before the interview demo:

1. âœ… **Environment Ready**
   - [ ] Virtual environment activated
   - [ ] All dependencies installed
   - [ ] spaCy model downloaded

2. âœ… **Ollama Running**
   - [ ] `ollama serve` running in background
   - [ ] `mistral:7b` model available

3. âœ… **Data Processed**
   - [ ] Run `python run.py --ingest` beforehand
   - [ ] Verify `data/processed/` folder has files

4. âœ… **Test Queries Ready**
   - "Who was Dr. B.R. Ambedkar?"
   - "What were Ambedkar's views on caste?"
   - "What role did Ambedkar play in the Indian Constitution?"
   - "What is the significance of education according to Ambedkar?"
   - "How did Ambedkar fight for social justice?"

5. âœ… **Quick Test**
   ```bash
   python run.py --load --query "Who was Ambedkar?"
   ```

## ğŸ”§ Troubleshooting

### Ollama Connection Error
```bash
# Make sure Ollama is running
ollama serve

# Check if model is available
ollama list

# Pull model if missing
ollama pull mistral:7b
```

### Memory Issues
- Reduce `max_tokens` in config
- Use smaller embedding model
- Process document in batches

### spaCy Model Not Found
```bash
python -m spacy download en_core_web_sm
```

### Slow Ingestion
- First run downloads embedding model (~90MB)
- Graph construction may take 5-10 minutes for 94-page PDF
- Subsequent loads are fast (uses cached data)

## ğŸ“š SEMRAG Implementation Details

### Algorithm 1: Semantic Chunking
- Sentence embeddings via `all-MiniLM-L6-v2`
- Cosine similarity grouping with threshold
- Buffer merging for context preservation
- Token limits enforced (max 1024, sub-chunks ~128)

### Equation 4: Local RAG Search
```
D_retrieved = Top_k({v âˆˆ V, g âˆˆ G | sim(v, Q+H) > Ï„_e âˆ§ sim(g, v) > Ï„_d})
```
- Entity similarity threshold Ï„_e = 0.3
- Chunk similarity threshold Ï„_d = 0.2

### Equation 5: Global RAG Search
```
D_retrieved = Top_k(â‹ƒ_{r âˆˆ R_Top-K(Q)} â‹ƒ_{c_i âˆˆ C_r} (â‹ƒ_{p_j âˆˆ c_i} (p_j, score(p_j, Q))))
```
- Community detection via Louvain algorithm
- Top-K community summaries
- Chunk scoring within communities

## ğŸ“„ License

This project is for educational purposes as part of an internship assignment.

## ğŸ™ Acknowledgments

- SEMRAG Research Paper
- Dr. B.R. Ambedkar's Works
- Ollama Team
- Sentence Transformers
